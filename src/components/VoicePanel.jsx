'use client'

import { useState, useRef, useEffect } from 'react'
import gptConfig from '@/config/gpt-config.json'
import voicePresets from '@/config/voice-presets.json'

export default function VoicePanel() {
  const [isRunning, setIsRunning] = useState(false)
  const [userText, setUserText] = useState('')
  const [assistantText, setAssistantText] = useState('')
  const [status, setStatus] = useState('HazÄ±r')
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [testMode, setTestMode] = useState(false)
  const [debugInfo, setDebugInfo] = useState([])
  const [currentStep, setCurrentStep] = useState('')
  const [processSteps, setProcessSteps] = useState([])
  const [currentPreset, setCurrentPreset] = useState('default')
  const [lastResponseAudio, setLastResponseAudio] = useState(null) // Son yanÄ±tÄ±n ses dosyasÄ±
  const [isReplaying, setIsReplaying] = useState(false) // Tekrar Ã§alma durumu
  const [fullResponseAudios, setFullResponseAudios] = useState([]) // TÃ¼m yanÄ±tÄ±n ses parÃ§alarÄ±
  const [currentResponseId, setCurrentResponseId] = useState(null) // Mevcut yanÄ±t ID'si
  const [lastUserSpeechTime, setLastUserSpeechTime] = useState(0) // Son kullanÄ±cÄ± konuÅŸma zamanÄ±

  const mediaRecorderRef = useRef(null)

  // Sayfa yenilendiÄŸinde tÃ¼m geÃ§miÅŸi temizle
  useEffect(() => {
    // Component mount olduÄŸunda tÃ¼m state'leri temizle
    setUserText('')
    setAssistantText('')
    setDebugInfo([])
    setProcessSteps([])
    setFullResponseAudios([])
    setLastResponseAudio(null)
    setCurrentResponseId(null)
    addDebugInfo('Sayfa yenilendi, tÃ¼m geÃ§miÅŸ temizlendi', 'info')
  }, []) // Sadece component mount olduÄŸunda Ã§alÄ±ÅŸ
  const audioQueueRef = useRef([])
  const isPlayingRef = useRef(false)
  const fileInputRef = useRef(null)
  const recordingTimeoutRef = useRef(null)
  const isMouseDownRef = useRef(false)

  // Debug logging
  const addDebugInfo = (message, type = 'info') => {
    const timestamp = new Date().toLocaleTimeString()
    setDebugInfo(prev => [...prev, { message, type, timestamp }])
    console.log(`[${timestamp}] ${type.toUpperCase()}: ${message}`)
  }

  // Zaman damgasÄ± ile detaylÄ± debug bilgisi
  const addDetailedDebugInfo = (message, type = 'info', details = {}) => {
    const now = new Date()
    const timestamp = now.toLocaleTimeString('tr-TR')
    const milliseconds = now.getMilliseconds().toString().padStart(3, '0')
    const fullTimestamp = `${timestamp}.${milliseconds}`
    
    setDebugInfo(prev => [...prev, { 
      message, 
      type, 
      timestamp: fullTimestamp,
      details 
    }])
  }

  // Process steps tracking
  const addProcessStep = (step, status = 'pending') => {
    setProcessSteps(prev => [...prev, { step, status, timestamp: new Date().toLocaleTimeString() }])
  }

  const updateProcessStep = (step, status) => {
    setProcessSteps(prev => prev.map(s => s.step === step ? { ...s, status } : s))
  }

  // Audio playback queue - Sequential playback (no overlap)
  const playNextAudio = () => {
    if (audioQueueRef.current.length > 0 && !isPlayingRef.current) {
      const playStartTime = Date.now()
      const audioId = `AUDIO_${playStartTime}_${Math.random().toString(36).substr(2, 9)}`
      
      isPlayingRef.current = true
      setIsSpeaking(true)
      
      // AI konuÅŸurken mikrofonu tamamen durdur ve kayÄ±t yapmayÄ± engelle
      if (isRecording) {
        stopRecording()
        addDebugInfo('AI konuÅŸuyor, mikrofon durduruldu', 'info')
      }
      
      // Mouse down durumunu da sÄ±fÄ±rla
      isMouseDownRef.current = false
      setIsRunning(false)
      
      const audioBlob = audioQueueRef.current.shift()
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      
      addDetailedDebugInfo(`ğŸ”Š SES Ã‡ALMA BAÅLADI`, 'info', {
        audioId,
        fileSize: `${audioBlob.size} bytes`,
        queueRemaining: audioQueueRef.current.length,
        startTime: new Date().toISOString()
      });
      
      audio.onended = () => {
        const playEndTime = Date.now()
        const playDuration = playEndTime - playStartTime
        
        addDetailedDebugInfo(`ğŸ”Š SES Ã‡ALMA TAMAMLANDI`, 'success', {
          audioId,
          playDuration: `${playDuration}ms`,
          queueRemaining: audioQueueRef.current.length
        });
        
        URL.revokeObjectURL(audioUrl)
        isPlayingRef.current = false
        setIsSpeaking(false)
        
        // Hemen bir sonraki sesi Ã§al (kesintisiz geÃ§iÅŸ)
        setTimeout(() => {
          playNextAudio()
        }, 50)
      }
      
      audio.onerror = (error) => {
        const playEndTime = Date.now()
        const playDuration = playEndTime - playStartTime
        
        addDetailedDebugInfo(`âŒ SES Ã‡ALMA HATASI`, 'error', {
          audioId,
          error: error.message || 'Unknown error',
          playDuration: `${playDuration}ms`
        });
        
        URL.revokeObjectURL(audioUrl)
        isPlayingRef.current = false
        setIsSpeaking(false)
        playNextAudio()
      }
      
      audio.play().catch((error) => {
        addDetailedDebugInfo(`âŒ SES OYNATMA HATASI: ${error.message}`, 'error', {
          audioId,
          error: error.message
        });
      });
    }
  }

  // Process TTS - Fixed order issue with detailed timing
  async function processTTS(text) {
    if (!text?.trim()) return;

    const startTime = Date.now()
    const ttsId = `TTS_${startTime}_${Math.random().toString(36).substr(2, 9)}`
    
    try {
      setCurrentStep('TTS: Ses Ã¼retiliyor...');
      addDetailedDebugInfo(`ğŸµ TTS BAÅLATILDI: "${text.trim()}"`, 'info', {
        ttsId,
        textLength: text.trim().length,
        preset: currentPreset,
        startTime: new Date().toISOString()
      });
      addProcessStep('TTS: Ses Ã¼retiliyor', 'in_progress');
      setStatus('Ses Ã¼retiliyor...');

      // ElevenLabs preset'i al
      const preset = voicePresets.presets['default'] // Sadece default preset kullanÄ±yoruz
      
      const apiStartTime = Date.now()
      addDetailedDebugInfo(`ğŸ“¡ ELEVENLABS API Ã‡AÄRISI BAÅLADI`, 'info', {
        ttsId,
        voice_id: preset.elevenlabs_voice_id,
        voice_settings: preset.voice_settings,
        provider: 'ElevenLabs'
      });
      
      const res = await fetch('/api/tts/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          text: text.trim(),
          voice_settings: preset.voice_settings
        }),
      });

      const apiEndTime = Date.now()
      const apiDuration = apiEndTime - apiStartTime
      
      addDetailedDebugInfo(`ğŸ“¡ API Ã‡AÄRISI TAMAMLANDI: ${res.status}`, res.ok ? 'success' : 'error', {
        ttsId,
        apiDuration: `${apiDuration}ms`,
        status: res.status
      });
      
      if (!res.ok) {
        const err = await res.text();
        addDetailedDebugInfo(`âŒ API HATASI: ${err}`, 'error', {
          ttsId,
          error: err,
          apiDuration: `${apiDuration}ms`
        });
        updateProcessStep('TTS: Ses Ã¼retiliyor', 'error');
        return;
      }

      const bufferStartTime = Date.now()
      const buf = await res.arrayBuffer();
      const blob = new Blob([buf], { type: 'audio/mpeg' });
      const bufferEndTime = Date.now()
      const bufferDuration = bufferEndTime - bufferStartTime
      
      // SÄ±raya ekle ve Ã§al
      audioQueueRef.current.push(blob);
      playNextAudio();
      
      // TÃ¼m yanÄ±tÄ±n ses parÃ§alarÄ±nÄ± kaydet (tekrar Ã§alma iÃ§in)
      setFullResponseAudios(prev => [...prev, {
        id: ttsId,
        text: text.trim(),
        audio: blob,
        timestamp: new Date().toISOString()
      }]);
      
      // Son yanÄ±tÄ±n ses dosyasÄ±nÄ± da kaydet (geriye uyumluluk iÃ§in)
      setLastResponseAudio(blob);
      
      const totalDuration = Date.now() - startTime
      
      addDetailedDebugInfo(`âœ… TTS TAMAMLANDI: ${blob.size} bytes - SIRAYA EKLENDÄ°`, 'success', {
        ttsId,
        fileSize: `${blob.size} bytes`,
        apiDuration: `${apiDuration}ms`,
        bufferDuration: `${bufferDuration}ms`,
        totalDuration: `${totalDuration}ms`,
        queuePosition: audioQueueRef.current.length
      });
      
      updateProcessStep('TTS: Ses Ã¼retiliyor', 'completed');
    } catch (e) {
      const totalDuration = Date.now() - startTime
      addDetailedDebugInfo(`âŒ TTS HATASI: ${e.message}`, 'error', {
        ttsId,
        error: e.message,
        totalDuration: `${totalDuration}ms`
      });
      updateProcessStep('TTS: Ses Ã¼retiliyor', 'error');
      console.error('OpenAI TTS error:', e);
    } finally {
      setStatus('HazÄ±r');
    }
  }

  // Process chat completion - Fixed TTS order
  const processChat = async (prompt) => {
    if (!prompt.trim()) {
      addDebugInfo('BoÅŸ input, yanÄ±t verilmiyor', 'warning');
      return;
    }

    // Input validation - sadece gerÃ§ek kullanÄ±cÄ± input'u kabul et
    const trimmedPrompt = prompt.trim();
    if (trimmedPrompt.length < 2) {
      addDebugInfo('Ã‡ok kÄ±sa input, yanÄ±t verilmiyor', 'warning');
      return;
    }

    // AI'nin kendi yanÄ±tlarÄ±nÄ± tekrar iÅŸlemesini engelle
    const aiResponsePatterns = [
      'Merhaba, ben Selin',
      'Size nasÄ±l yardÄ±mcÄ± olabilirim',
      'NasÄ±lsÄ±nÄ±z',
      'SaÃ§ Ekimi Merkezi',
      'FUE tekniÄŸi',
      'FUT tekniÄŸi',
      'DHI tekniÄŸi',
      'Sapphire FUE',
      'folikÃ¼ler Ã¼nite',
      'saÃ§ ekimi konusunda',
      'deneyimliyiz',
      'baÅŸarÄ± hikayesi',
      'kliniÄŸe yÃ¶nlendir',
      'randevu almak',
      'endÃ¼ÅŸelenmeyin',
      'gÃ¼ven verici'
    ];
    
    const isAiResponse = aiResponsePatterns.some(pattern => 
      trimmedPrompt.toLowerCase().includes(pattern.toLowerCase())
    );
    
    if (isAiResponse) {
      addDebugInfo('AI kendi yanÄ±tÄ±nÄ± tekrar iÅŸlemeye Ã§alÄ±ÅŸÄ±yor, engellendi', 'warning');
      return;
    }

    // Yeni yanÄ±t baÅŸladÄ±ÄŸÄ±nda Ã¶nceki ses parÃ§alarÄ±nÄ± temizle
    const responseId = Date.now().toString()
    setCurrentResponseId(responseId)
    setFullResponseAudios([])
    setLastResponseAudio(null)

    try {
      setCurrentStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor...');
      addDebugInfo(`Chat baÅŸlatÄ±lÄ±yor: "${prompt.trim()}"`, 'info');
      addProcessStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor', 'in_progress');
      setStatus('AI yanÄ±tÄ± alÄ±nÄ±yor...');
      setIsProcessing(true);
      setAssistantText('');

      // Mevcut preset'i al
      const preset = voicePresets.presets[currentPreset]
      
      // GPT config'i preset ile birleÅŸtir
      const systemPrompt = `${gptConfig.system_prompt}\n\n${preset.style_instructions}\n\n${gptConfig.voice_guidelines.sentence_structure}`
      
      const response = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          prompt: prompt.trim(),
          system_prompt: systemPrompt,
          max_tokens: gptConfig.technical_instructions.max_tokens,
          temperature: gptConfig.technical_instructions.temperature
        })
      });

      addDebugInfo(`Chat API yanÄ±tÄ±: ${response.status}`, response.ok ? 'success' : 'error');

      if (!response.ok) {
        const errorText = await response.text();
        addDebugInfo(`Chat hatasÄ±: ${errorText}`, 'error');
        updateProcessStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor', 'error');
        throw new Error('Chat request failed');
      }

      // Read stream and collect full response - REALTIME STREAMING
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';
      let fullResponse = '';
      let sentenceBuffer = '';

      const getChunkText = (payload) => {
        try {
          const d = JSON.parse(payload);
          return (
            d?.text ??
            d?.delta?.content?.[0]?.text ??
            d?.choices?.[0]?.delta?.content?.[0]?.text ??
            d?.choices?.[0]?.text ??
            d?.message?.content ??
            ''
          );
        } catch {
          return payload;
        }
      };

      addDebugInfo('Chat stream baÅŸlatÄ±ldÄ± - REALTIME MODE', 'info');

      while (true) {
        const { done, value } = await reader.read();
        if (done) {
          addDebugInfo('Chat stream tamamlandÄ±', 'success');
          break;
        }

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          if (!line.startsWith('data: ')) continue;
          const payload = line.slice(6).trim();

          if (!payload || payload === '[DONE]') continue;

          const chunk = getChunkText(payload);
          if (!chunk) continue;

          addDebugInfo(`Chat token alÄ±ndÄ±: "${chunk}"`, 'info');
          fullResponse += chunk;
          setAssistantText(fullResponse);

          // REALTIME: Her kelimeyi iÅŸle
          sentenceBuffer += chunk;
          
          // YÃ¶ntem 1: Sadece noktalama iÅŸaretlerinde bÃ¶l (en gÃ¼venli)
          if (/[.!?]\s/.test(sentenceBuffer)) {
            const sentences = sentenceBuffer.split(/([.!?]\s)/);
            let completeSentence = '';
            
            for (let i = 0; i < sentences.length - 1; i += 2) {
              if (sentences[i] && sentences[i + 1]) {
                completeSentence = sentences[i].trim() + sentences[i + 1];
                if (completeSentence && completeSentence.length > 10) { // En az 10 karakter
                  // Hemen TTS'ye gÃ¶nder
                  addDebugInfo(`Realtime TTS: "${completeSentence}"`, 'success');
                  updateProcessStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor', 'completed');
                  addProcessStep('TTS: Ses Ã¼retiliyor', 'pending');
                  await processTTS(completeSentence);
                }
              }
            }
            
            // Kalan kÄ±smÄ± buffer'da tut
            sentenceBuffer = sentences[sentences.length - 1] || '';
          }
          
          // YÃ¶ntem 2: Ã‡ok uzun cÃ¼mlelerde (150+ karakter) son virgÃ¼lden bÃ¶l
          else if (sentenceBuffer.length > 150 && sentenceBuffer.includes(',')) {
            const lastCommaIndex = sentenceBuffer.lastIndexOf(',');
            if (lastCommaIndex > sentenceBuffer.length * 0.8) { // Son %20'de ise
              const firstPart = sentenceBuffer.substring(0, lastCommaIndex + 1).trim();
              const remainingPart = sentenceBuffer.substring(lastCommaIndex + 1).trim();
              
              if (firstPart.length > 30) {
                addDebugInfo(`Uzun cÃ¼mle TTS: "${firstPart}"`, 'success');
                await processTTS(firstPart);
                sentenceBuffer = remainingPart;
              }
            }
          }
          
          // YÃ¶ntem 3: Ã‡ok Ã§ok uzun cÃ¼mlelerde (200+ karakter) orta noktadan bÃ¶l
          else if (sentenceBuffer.length > 200) {
            const midPoint = Math.floor(sentenceBuffer.length / 2);
            const spaceIndex = sentenceBuffer.indexOf(' ', midPoint);
            
            if (spaceIndex > midPoint - 20 && spaceIndex < midPoint + 20) {
              const firstPart = sentenceBuffer.substring(0, spaceIndex).trim();
              const remainingPart = sentenceBuffer.substring(spaceIndex).trim();
              
              if (firstPart.length > 50) {
                addDebugInfo(`Ã‡ok uzun cÃ¼mle TTS: "${firstPart}"`, 'success');
                await processTTS(firstPart);
                sentenceBuffer = remainingPart;
              }
            }
          }
        }
      }

      // Kalan buffer'Ä± da akÄ±llÄ± iÅŸle
      if (sentenceBuffer.trim() && sentenceBuffer.trim().length > 5) {
        addDebugInfo(`Son TTS: "${sentenceBuffer.trim()}"`, 'success');
        await processTTS(sentenceBuffer.trim());
      } else if (sentenceBuffer.trim()) {
        // Ã‡ok kÄ±sa kalan kÄ±sÄ±m varsa bir sonraki yanÄ±tla birleÅŸtir
        addDebugInfo(`KÄ±sa buffer bekletiliyor: "${sentenceBuffer.trim()}"`, 'info');
      }

    } catch (error) {
      addDebugInfo(`Chat error: ${error.message}`, 'error');
      updateProcessStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor', 'error');
      console.error('Chat error:', error);
      setStatus('Hata: ' + error.message);
    } finally {
      setIsProcessing(false);
      setCurrentStep('');
      setStatus('HazÄ±r');
    }
  };

  // Process STT with OpenAI Whisper
  const processSTT = async (audioBlob) => {
    try {
      setCurrentStep('STT: OpenAI Whisper ile konuÅŸma iÅŸleniyor...');
      addDebugInfo(`OpenAI Whisper STT baÅŸlatÄ±lÄ±yor: ${audioBlob.size} bytes, ${audioBlob.type}`, 'info');
      addProcessStep('STT: KonuÅŸma iÅŸleniyor', 'in_progress');
      setStatus('KonuÅŸma iÅŸleniyor...');
      
      const formData = new FormData()
      formData.append('audio', audioBlob, 'audio.webm')

      addDebugInfo('OpenAI Whisper API Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor...', 'info')

      const response = await fetch('/api/stt/stream', {
        method: 'POST',
        body: formData
      })

      addDebugInfo(`OpenAI Whisper API yanÄ±tÄ±: ${response.status}`, response.ok ? 'success' : 'error')

      if (response.ok) {
        const result = await response.json()
        addDebugInfo(`OpenAI Whisper sonucu: ${JSON.stringify(result)}`, 'success')
        
        if (result.text) {
          const currentTime = Date.now()
          const timeSinceLastUserSpeech = currentTime - lastUserSpeechTime
          
          // EÄŸer AI konuÅŸuyorsa ve son kullanÄ±cÄ± konuÅŸmasÄ±ndan 5 saniye geÃ§memiÅŸse, bu muhtemelen AI'nin kendi sesi
          if (isSpeaking && timeSinceLastUserSpeech < 5000) {
            addDebugInfo('AI konuÅŸurken gelen ses, muhtemelen AI\'nin kendi sesi - yok sayÄ±lÄ±yor', 'warning')
            updateProcessStep('STT: KonuÅŸma iÅŸleniyor', 'cancelled');
            return
          }
          
          addDebugInfo(`Metin alÄ±ndÄ±: "${result.text}"`, 'success')
          setUserText(result.text)
          setLastUserSpeechTime(currentTime)
          updateProcessStep('STT: KonuÅŸma iÅŸleniyor', 'completed');
          addProcessStep('Chat: AI yanÄ±tÄ± alÄ±nÄ±yor', 'pending');
          await processChat(result.text)
        } else {
          addDebugInfo('OpenAI Whisper sonucunda metin bulunamadÄ±', 'warning')
          updateProcessStep('STT: KonuÅŸma iÅŸleniyor', 'error');
        }
      } else {
        const errorText = await response.text()
        addDebugInfo(`OpenAI Whisper hatasÄ±: ${errorText}`, 'error')
        updateProcessStep('STT: KonuÅŸma iÅŸleniyor', 'error');
        setStatus('STT hatasÄ±: ' + errorText)
      }
    } catch (error) {
      addDebugInfo(`OpenAI Whisper error: ${error.message}`, 'error')
      updateProcessStep('STT: KonuÅŸma iÅŸleniyor', 'error');
      console.error('OpenAI Whisper error:', error)
      setStatus('Hata: ' + error.message)
    }
  }

  // Start recording with Web Speech API
  const startRecording = async () => {
    try {
      if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        addDebugInfo('Web Speech API desteklenmiyor, MediaRecorder kullanÄ±lÄ±yor', 'warning')
        startRecordingWithMediaRecorder()
        return
      }

      addDebugInfo('Web Speech API ile kayÄ±t baÅŸlatÄ±lÄ±yor...', 'info')
      setStatus('Web Speech API ile dinleniyor...')
      setIsRecording(true)

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
      const recognition = new SpeechRecognition()
      
      recognition.lang = 'tr-TR'
      recognition.continuous = true
      recognition.interimResults = true

      recognition.onresult = (event) => {
        let finalTranscript = ''
        let interimTranscript = ''

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }

        if (interimTranscript) {
          setUserText(interimTranscript)
          addDebugInfo(`Interim: "${interimTranscript}"`, 'info')
        }

        if (finalTranscript) {
          addDebugInfo(`Final: "${finalTranscript}"`, 'success')
          setUserText(finalTranscript)
          processChat(finalTranscript)
        }
      }

      recognition.onerror = (event) => {
        addDebugInfo(`Web Speech hatasÄ±: ${event.error}`, 'error')
        setStatus('HazÄ±r')
        setIsRecording(false)
      }

      recognition.onend = () => {
        setStatus('HazÄ±r')
        setIsRecording(false)
      }

      recognition.start()

    } catch (error) {
      addDebugInfo(`Recording error: ${error.message}`, 'error')
      setStatus('Mikrofon eriÅŸim hatasÄ±: ' + error.message)
    }
  }

  // Fallback to MediaRecorder
  const startRecordingWithMediaRecorder = async () => {
    try {
      setStatus('Mikrofon izni alÄ±nÄ±yor...')
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: { 
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      })

    const mediaRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm'
    })

      mediaRecorderRef.current = mediaRecorder
      setIsRecording(true)
      setStatus('OpenAI Whisper ile kayÄ±t yapÄ±lÄ±yor...')

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          addDebugInfo(`MediaRecorder data: ${event.data.size} bytes`, 'info')
          processSTT(event.data)
        }
      }

      mediaRecorder.start(1000)

    } catch (error) {
      console.error('MediaRecorder error:', error)
      setStatus('Mikrofon eriÅŸim hatasÄ±: ' + error.message)
    }
  }

  // Stop recording
  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current.stream.getTracks().forEach(track => track.stop())
      mediaRecorderRef.current = null
    }
    setIsRecording(false)
    setStatus('HazÄ±r')
  }

  // Mouse events for hold-to-record
  const handleMouseDown = () => {
    if (isProcessing || isSpeaking) {
      addDebugInfo('AI konuÅŸuyor, kayÄ±t yapÄ±lamÄ±yor', 'warning')
      return;
    }
    
    // EÄŸer zaten kayÄ±t yapÄ±lÄ±yorsa, yeni kayÄ±t baÅŸlatma
    if (isRecording) {
      addDebugInfo('Zaten kayÄ±t yapÄ±lÄ±yor', 'warning')
      return;
    }
    
    isMouseDownRef.current = true;
    setIsRunning(true);
    startRecording();
    
    // Auto-stop after 10 seconds if no speech detected
    recordingTimeoutRef.current = setTimeout(() => {
      if (isMouseDownRef.current) {
        handleMouseUp();
      }
    }, 10000);
  }

  const handleMouseUp = () => {
    isMouseDownRef.current = false;
    if (recordingTimeoutRef.current) {
      clearTimeout(recordingTimeoutRef.current);
    }
    stopRecording();
    setIsRunning(false);
  }

  // Test functions
  const handleFileUpload = async (event) => {
    const file = event.target.files[0]
    if (file) {
      setStatus('Dosya iÅŸleniyor...')
      addDebugInfo(`Dosya seÃ§ildi: ${file.name}, ${file.size} bytes, ${file.type}`, 'info')
      await processSTT(file)
    }
  }

  const testWithText = () => {
    const testText = "Merhaba, bu bir test mesajÄ±dÄ±r. NasÄ±lsÄ±n?"
    addDebugInfo(`Test metni: "${testText}"`, 'info')
    // setUserText Ã§aÄŸrÄ±lmÄ±yor - sadece AI'ye gÃ¶nder
    processChat(testText)
  }

  const testWebSpeech = () => {
    if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
      addDebugInfo('Web Speech API desteklenmiyor', 'error')
      return
    }

    addDebugInfo('Web Speech API baÅŸlatÄ±lÄ±yor...', 'info')
    setStatus('Web Speech API ile dinleniyor...')

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
    const recognition = new SpeechRecognition()
    
    recognition.lang = 'tr-TR'
    recognition.continuous = false
    recognition.interimResults = false

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript
      addDebugInfo(`Web Speech sonucu: "${transcript}"`, 'success')
      // setUserText Ã§aÄŸrÄ±lmÄ±yor - sadece AI'ye gÃ¶nder
      processChat(transcript)
    }

    recognition.onerror = (event) => {
      addDebugInfo(`Web Speech hatasÄ±: ${event.error}`, 'error')
      setStatus('HazÄ±r')
    }

    recognition.onend = () => {
      setStatus('HazÄ±r')
    }

    recognition.start()
  }

  const testTTS = () => {
    const testText = "OpenAI TTS testi: Merhaba, bu OpenAI TTS ile Ã¼retilen bir ses testidir."
    addDebugInfo(`OpenAI TTS Test: "${testText}"`, 'info')
    // setUserText Ã§aÄŸrÄ±lmÄ±yor - sadece TTS testi
    processTTS(testText)
  }

  // Tekrar Ã§alma fonksiyonu - TÃ¼m yanÄ±tÄ± sÄ±rayla Ã§al
  const replayLastResponse = () => {
    if (fullResponseAudios.length === 0) {
      addDebugInfo('Tekrar Ã§alÄ±nacak ses dosyasÄ± bulunamadÄ±', 'warning')
      return
    }

    setIsReplaying(true)
    addDebugInfo(`TÃ¼m yanÄ±t tekrar Ã§alÄ±nÄ±yor... (${fullResponseAudios.length} parÃ§a)`, 'info')
    
    // TÃ¼m ses parÃ§alarÄ±nÄ± sÄ±rayla Ã§al
    let currentIndex = 0
    
    const playNextPart = () => {
      if (currentIndex >= fullResponseAudios.length) {
        setIsReplaying(false)
        addDebugInfo('TÃ¼m yanÄ±t tekrar Ã§alma tamamlandÄ±', 'success')
        return
      }
      
      const audioPart = fullResponseAudios[currentIndex]
      const audioUrl = URL.createObjectURL(audioPart.audio)
      const audio = new Audio(audioUrl)
      
      addDebugInfo(`ParÃ§a ${currentIndex + 1}/${fullResponseAudios.length}: "${audioPart.text}"`, 'info')
      
      audio.onended = () => {
        URL.revokeObjectURL(audioUrl)
        currentIndex++
        // Hemen bir sonraki parÃ§ayÄ± Ã§al (kesintisiz geÃ§iÅŸ)
        setTimeout(playNextPart, 50)
      }
      
      audio.onerror = () => {
        URL.revokeObjectURL(audioUrl)
        addDebugInfo(`ParÃ§a ${currentIndex + 1} Ã§alma hatasÄ±`, 'error')
        currentIndex++
        setTimeout(playNextPart, 50)
      }
      
      audio.play().catch((error) => {
        addDebugInfo(`ParÃ§a ${currentIndex + 1} oynatma hatasÄ±: ${error.message}`, 'error')
        currentIndex++
        setTimeout(playNextPart, 50)
      })
    }
    
    playNextPart()
  }

  // Clear all data
  const clearAll = () => {
    setDebugInfo([])
    setProcessSteps([])
    setUserText('')
    setAssistantText('')
    setLastResponseAudio(null)
    setFullResponseAudios([])
    setCurrentResponseId(null)
    audioQueueRef.current = []
    isPlayingRef.current = false
    setIsSpeaking(false)
    setIsReplaying(false)
  }

  return (
    <div className="bg-white rounded-2xl shadow-xl p-6 border border-gray-100">
      <div className="text-center mb-6">
        <h2 className="text-2xl font-bold text-gray-900 mb-2">ElevenLabs Sesli Asistan</h2>
        <p className="text-gray-600">Mikrofonla konuÅŸun, OpenAI GPT + Whisper + ElevenLabs TTS ile yanÄ±tlasÄ±n</p>
        <div className="mt-2 flex justify-center space-x-2 text-xs text-gray-500">
          <span className="bg-blue-100 text-blue-800 px-2 py-1 rounded">GPT-4o</span>
          <span className="bg-green-100 text-green-800 px-2 py-1 rounded">Whisper STT</span>
          <span className="bg-purple-100 text-purple-800 px-2 py-1 rounded">ElevenLabs TTS</span>
        </div>
      </div>

      {/* Test Mode Toggle */}
      <div className="text-center mb-4 space-x-2">
        <button
          onClick={() => setTestMode(!testMode)}
          className="px-4 py-2 text-sm bg-gray-100 hover:bg-gray-200 rounded-lg transition-colors"
        >
          {testMode ? 'CanlÄ± KayÄ±t Modu' : 'Test Modu (Dosya YÃ¼kle)'}
        </button>
        <button
          onClick={testWithText}
          className="px-4 py-2 text-sm bg-green-100 hover:bg-green-200 text-green-800 rounded-lg transition-colors"
        >
          Metin Testi
        </button>
        <button
          onClick={testWebSpeech}
          className="px-4 py-2 text-sm bg-blue-100 hover:bg-blue-200 text-blue-800 rounded-lg transition-colors"
        >
          Web Speech Testi
        </button>
        <button
          onClick={testTTS}
          className="px-4 py-2 text-sm bg-purple-100 hover:bg-purple-200 text-purple-800 rounded-lg transition-colors"
        >
          ElevenLabs TTS Testi
        </button>
      </div>

      {/* ElevenLabs Voice Info */}
      <div className="mb-6">
        <h3 className="text-sm font-semibold text-gray-700 mb-3 text-center">ElevenLabs Ses Sistemi</h3>
        <div className="bg-gradient-to-r from-purple-50 to-blue-50 rounded-lg p-4 border border-purple-200">
          <div className="text-center">
            <div className="text-2xl mb-2">ğŸ™ï¸</div>
            <div className="font-medium text-gray-800">{voicePresets.presets.default.name}</div>
            <div className="text-sm text-gray-600 mt-1">{voicePresets.presets.default.description}</div>
            <div className="text-xs text-gray-500 mt-2">
              Voice ID: <span className="font-mono bg-gray-100 px-2 py-1 rounded">{voicePresets.elevenlabs_config.voice_id}</span>
            </div>
          </div>
        </div>
      </div>

      {/* Control Button */}
      <div className="text-center mb-6">
        {testMode ? (
          <div>
            <input
              ref={fileInputRef}
              type="file"
              accept="audio/*"
              onChange={handleFileUpload}
              className="hidden"
            />
            <button
              onClick={() => fileInputRef.current?.click()}
              disabled={isProcessing}
              className="w-24 h-24 rounded-full text-white font-bold text-lg transition-all duration-200 transform hover:scale-105 disabled:opacity-50 disabled:cursor-not-allowed bg-gradient-to-r from-green-500 to-blue-500 hover:from-green-600 hover:to-blue-600 shadow-lg"
            >
              ğŸ“
            </button>
            <p className="mt-2 text-sm text-gray-600">
              Ses dosyasÄ± yÃ¼kleyin
            </p>
          </div>
        ) : (
          <div>
            <button
              onMouseDown={handleMouseDown}
              onMouseUp={handleMouseUp}
              onMouseLeave={handleMouseUp}
              disabled={isProcessing || isSpeaking}
              className={`w-24 h-24 rounded-full text-white font-bold text-lg transition-all duration-200 transform hover:scale-105 disabled:opacity-50 disabled:cursor-not-allowed select-none ${
                isRunning 
                  ? 'bg-red-500 hover:bg-red-600 shadow-lg' 
                  : isSpeaking
                  ? 'bg-yellow-500 shadow-lg'
                  : 'bg-gradient-to-r from-indigo-500 to-purple-500 hover:from-indigo-600 hover:to-purple-600 shadow-lg'
              }`}
            >
              {isRunning ? 'â– ' : isSpeaking ? 'ğŸ”Š' : 'â—'}
            </button>
            <p className="mt-2 text-sm text-gray-600">
              {isRunning ? 'BasÄ±lÄ± tutun ve konuÅŸun' : 'BasÄ±lÄ± tutarak konuÅŸun'}
            </p>
          </div>
        )}
      </div>

      {/* Status */}
      <div className="text-center mb-4">
        <div className={`inline-flex items-center px-3 py-1 rounded-full text-sm font-medium ${
          isRecording ? 'bg-red-100 text-red-800' :
          isProcessing ? 'bg-yellow-100 text-yellow-800' :
          isSpeaking ? 'bg-green-100 text-green-800' :
          'bg-gray-100 text-gray-800'
        }`}>
          <div className={`w-2 h-2 rounded-full mr-2 ${
            isRecording ? 'bg-red-500 animate-pulse' :
            isProcessing ? 'bg-yellow-500 animate-pulse' :
            isSpeaking ? 'bg-green-500 animate-pulse' :
            'bg-gray-500'
          }`}></div>
          {status}
        </div>
      </div>

      {/* Process Steps - New Beautiful Debug Panel */}

      {/* Text Areas */}
      <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
        {/* User Text */}
        <div>
          <h3 className="text-sm font-semibold text-gray-700 mb-2 flex items-center">
            <svg className="w-4 h-4 mr-2 text-green-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
            </svg>
            KullanÄ±cÄ± (OpenAI Whisper)
          </h3>
          <div className="bg-gray-50 rounded-lg p-3 min-h-[100px] border">
            <p className="text-gray-800 text-sm leading-relaxed">
              {userText || 'KonuÅŸmanÄ±z OpenAI Whisper ile burada gÃ¶rÃ¼necek...'}
            </p>
          </div>
        </div>

        {/* Assistant Text */}
        <div>
          <div className="flex items-center justify-between mb-2">
            <h3 className="text-sm font-semibold text-gray-700 flex items-center">
              <svg className="w-4 h-4 mr-2 text-blue-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z" />
              </svg>
              Asistan (OpenAI GPT-4o)
            </h3>
            {fullResponseAudios.length > 0 && (
              <button
                onClick={replayLastResponse}
                disabled={isReplaying || isSpeaking}
                className={`flex items-center space-x-1 px-3 py-1 rounded-lg text-xs font-medium transition-colors ${
                  isReplaying || isSpeaking
                    ? 'bg-gray-200 text-gray-500 cursor-not-allowed'
                    : 'bg-blue-500 text-white hover:bg-blue-600 active:bg-blue-700'
                }`}
              >
                {isReplaying ? (
                  <>
                    <svg className="w-3 h-3 animate-spin" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15" />
                    </svg>
                    <span>Ã‡alÄ±yor...</span>
                  </>
                ) : (
                  <>
                    <svg className="w-3 h-3" fill="currentColor" viewBox="0 0 24 24">
                      <path d="M8 5v14l11-7z"/>
                    </svg>
                    <span>Tekrar Ã‡al ({fullResponseAudios.length})</span>
                  </>
                )}
              </button>
            )}
          </div>
          <div className="bg-gray-50 rounded-lg p-3 min-h-[100px] border">
            <p className="text-gray-800 text-sm leading-relaxed">
              {assistantText || 'OpenAI GPT yanÄ±tÄ± burada gÃ¶rÃ¼necek...'}
            </p>
          </div>
        </div>
      </div>

      {/* Current Step */}
      {currentStep && (
        <div className="mt-4 p-3 bg-blue-50 border border-blue-200 rounded-lg">
          <div className="flex items-center">
            <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-blue-600 mr-2"></div>
            <span className="text-sm text-blue-800 font-medium">{currentStep}</span>
          </div>
        </div>
      )}

      {/* Process Steps - Scrollable */}
      {processSteps.length > 0 && (
        <div className="mt-6">
          <h3 className="text-sm font-semibold text-gray-700 mb-3 flex items-center">
            <svg className="w-4 h-4 mr-2 text-indigo-500" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z" />
            </svg>
            Ä°ÅŸlem AdÄ±mlarÄ±
          </h3>
          <div className="bg-gray-50 rounded-lg p-4 max-h-48 overflow-y-auto border">
            <div className="space-y-2">
              {processSteps.map((step, index) => (
                <div key={index} className="flex items-center p-3 bg-white rounded-lg border shadow-sm">
                  <div className={`w-6 h-6 rounded-full flex items-center justify-center mr-3 flex-shrink-0 ${
                    step.status === 'completed' ? 'bg-green-500' :
                    step.status === 'in_progress' ? 'bg-blue-500 animate-pulse' :
                    step.status === 'error' ? 'bg-red-500' :
                    'bg-gray-400'
                  }`}>
                    {step.status === 'completed' ? (
                      <svg className="w-4 h-4 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M5 13l4 4L19 7" />
                      </svg>
                    ) : step.status === 'in_progress' ? (
                      <div className="w-3 h-3 bg-white rounded-full animate-pulse"></div>
                    ) : step.status === 'error' ? (
                      <svg className="w-4 h-4 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
                      </svg>
                    ) : (
                      <div className="w-2 h-2 bg-white rounded-full"></div>
                    )}
                  </div>
                  <div className="flex-1 min-w-0">
                    <div className="text-sm font-medium text-gray-900 truncate">{step.step}</div>
                    <div className="text-xs text-gray-500">{step.timestamp}</div>
                  </div>
                </div>
              ))}
            </div>
          </div>
        </div>
      )}

      {/* Debug Panel */}
      <div className="mt-6">
        <div className="flex justify-between items-center mb-2">
          <h3 className="text-sm font-semibold text-gray-700">Debug Bilgileri</h3>
          <div className="space-x-2">
            <button
              onClick={clearAll}
              className="text-xs text-gray-500 hover:text-gray-700 px-2 py-1 rounded hover:bg-gray-100"
            >
              TÃ¼mÃ¼nÃ¼ Temizle
            </button>
            <button
              onClick={() => setDebugInfo([])}
              className="text-xs text-gray-500 hover:text-gray-700 px-2 py-1 rounded hover:bg-gray-100"
            >
              Debug Temizle
            </button>
          </div>
        </div>
        <div className="bg-gray-900 text-green-400 p-3 rounded-lg text-xs font-mono max-h-60 overflow-y-auto">
          {debugInfo.length === 0 ? (
            <div className="text-gray-500">Debug bilgileri burada gÃ¶rÃ¼necek...</div>
          ) : (
            debugInfo.map((info, index) => (
              <div key={index} className={`mb-2 p-2 rounded border-l-2 ${
                info.type === 'error' ? 'text-red-400 border-red-500 bg-red-900/20' :
                info.type === 'success' ? 'text-green-400 border-green-500 bg-green-900/20' :
                info.type === 'warning' ? 'text-yellow-400 border-yellow-500 bg-yellow-900/20' :
                'text-blue-400 border-blue-500 bg-blue-900/20'
              }`}>
                <div className="flex items-start justify-between">
                  <div className="flex-1">
                    <div className="text-gray-500 text-xs mb-1">[{info.timestamp}]</div>
                    <div className="font-medium">{info.message}</div>
                    {info.details && (
                      <div className="mt-2 text-xs text-gray-300 bg-black/30 p-2 rounded border">
                        <div className="grid grid-cols-2 gap-1">
                          {Object.entries(info.details).map(([key, value]) => (
                            <div key={key} className="flex">
                              <span className="text-gray-400 font-mono">{key}:</span>
                              <span className="ml-1 text-white font-medium">
                                {typeof value === 'object' ? JSON.stringify(value) : String(value)}
                              </span>
                            </div>
                          ))}
                        </div>
                      </div>
                    )}
                  </div>
                  <div className={`ml-2 px-2 py-1 rounded text-xs font-medium ${
                    info.type === 'error' ? 'bg-red-500/20 text-red-300' :
                    info.type === 'success' ? 'bg-green-500/20 text-green-300' :
                    info.type === 'warning' ? 'bg-yellow-500/20 text-yellow-300' :
                    'bg-blue-500/20 text-blue-300'
                  }`}>
                    {info.type.toUpperCase()}
                  </div>
                </div>
              </div>
            ))
          )}
        </div>
      </div>

      {/* Instructions */}
      <div className="mt-6 text-center">
        <p className="text-xs text-gray-500">
          Mikrofon butonunu basÄ±lÄ± tutarak konuÅŸun, bÄ±raktÄ±ÄŸÄ±nÄ±zda AI yanÄ±tlayacak
        </p>
      </div>
    </div>
  )
}